"""
LLM API è¿æ¥æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ API Keyã€Base URL å’Œæ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®
"""
import os
from openai import OpenAI

# é…ç½®ï¼ˆä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
LLM_API_KEY = os.getenv("LLM_API_KEY", "x")
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://api-inference.modelscope.cn/v1")
LLM_MODEL = os.getenv("LLM_MODEL", "Qwen/Qwen3-235B-A22B-Instruct-2507")

def test_llm_connection():
    print("=" * 50)
    print("ğŸ”§ LLM API è¿æ¥æµ‹è¯•")
    print("=" * 50)
    print(f"ğŸ“ Base URL: {LLM_BASE_URL}")
    print(f"ğŸ¤– Model: {LLM_MODEL}")
    print(f"ğŸ”‘ API Key: {LLM_API_KEY[:8]}...{LLM_API_KEY[-4:]}" if len(LLM_API_KEY) > 12 else "âš ï¸ API Key å¤ªçŸ­æˆ–æœªè®¾ç½®")
    print("=" * 50)
    
    if LLM_API_KEY == "your-api-key-here":
        print("âŒ é”™è¯¯: è¯·è®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡")
        print("   ç¤ºä¾‹: $env:LLM_API_KEY = 'sk-xxx'")
        return False
    
    try:
        client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
        
        print("\nğŸ“¤ å‘é€æµ‹è¯•è¯·æ±‚...")
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"}],
            temperature=0.1,
            max_tokens=100
        )
        
        print("\nğŸ“¥ å“åº”è¯¦æƒ…:")
        print(f"   - Choices æ•°é‡: {len(response.choices)}")
        
        if response.choices:
            choice = response.choices[0]
            print(f"   - Finish Reason: {choice.finish_reason}")
            print(f"   - Content: {choice.message.content}")
            print("\nâœ… LLM API è¿æ¥æ­£å¸¸!")
            return True
        else:
            print("\nâŒ é”™è¯¯: API è¿”å›ç©ºçš„ choices")
            print("   å¯èƒ½åŸå› :")
            print("   1. æ¨¡å‹åç§°é”™è¯¯")
            print("   2. è´¦æˆ·ä½™é¢ä¸è¶³")
            print("   3. API è¯·æ±‚è¢«æ‹’ç»")
            return False
            
    except Exception as e:
        print(f"\nâŒ è¿æ¥å¤±è´¥: {e}")
        print("\n   å¯èƒ½åŸå› :")
        print("   1. API Key æ— æ•ˆæˆ–è¿‡æœŸ")
        print("   2. Base URL é”™è¯¯")
        print("   3. ç½‘ç»œè¿æ¥é—®é¢˜")
        return False

def test_json_mode():
    """æµ‹è¯• JSON æ¨¡å¼æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("\n" + "=" * 50)
    print("ğŸ”§ JSON æ¨¡å¼æµ‹è¯•")
    print("=" * 50)
    
    try:
        client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
        
        print("\nğŸ“¤ å‘é€ JSON æ ¼å¼è¯·æ±‚...")
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": "è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«nameå’Œageå­—æ®µ"}],
            response_format={"type": "json_object"},
            temperature=0.1
        )
        
        if response.choices:
            content = response.choices[0].message.content
            print(f"   - Raw Response: {content}")
            
            import json
            result = json.loads(content)
            print(f"   - Parsed: {result}")
            print("\nâœ… JSON æ¨¡å¼æ­£å¸¸!")
            return True
        else:
            print("\nâŒ JSON æ¨¡å¼æµ‹è¯•å¤±è´¥: ç©ºå“åº”")
            return False
            
    except Exception as e:
        print(f"\nâŒ JSON æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = test_llm_connection()
    if success:
        test_json_mode()
